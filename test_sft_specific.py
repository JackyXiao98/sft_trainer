#!/usr/bin/env python3
"""
æµ‹è¯• SFT ç‰¹å®šçš„æ•°æ®å¤„ç†é—®é¢˜
"""

import torch
from transformers import AutoTokenizer
from datasets import Dataset

def create_mock_dataset():
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†"""
    # åŸºäºä½ æä¾›çš„çœŸå®æ•°æ®æ ·æœ¬
    sample_data = {
        'text': '<|im_start|>user\n<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nğŸ’•<|im_end|>\n',
        'messages': [{'content': '', 'role': 'user'}, {'content': 'ğŸ’•', 'role': 'assistant'}],
        'original_comment_id': '7479649524542620434',
        'interval': '[0.0, 0.1)',
        'dataset': 'tiktok',
        'category': 'comments',
        'language': 'en',
        'language_score': 0.7255634665489197,
        'token_num': 2
    }
    
    # åˆ›å»ºå¤šä¸ªæ ·æœ¬
    data = []
    for i in range(5):
        sample = sample_data.copy()
        sample['original_comment_id'] = f'test_id_{i}'
        data.append(sample)
    
    return Dataset.from_list(data)

def test_tokenizer_with_dataset():
    """æµ‹è¯• tokenizer ä¸æ•°æ®é›†çš„äº¤äº’"""
    print("=== æµ‹è¯• Tokenizer ä¸æ•°æ®é›†äº¤äº’ ===")
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
        dataset = create_mock_dataset()
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        print(f"æ•°æ®é›†å­—æ®µ: {list(dataset[0].keys())}")
        print(f"æ–‡æœ¬æ ·æœ¬: {dataset[0]['text'][:100]}...")
        
        # ä½¿ç”¨ GPT2 tokenizer è¿›è¡Œæµ‹è¯•
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç§»é™¤ token_type_ids
        if hasattr(tokenizer, 'model_input_names'):
            if 'token_type_ids' in tokenizer.model_input_names:
                tokenizer.model_input_names = [name for name in tokenizer.model_input_names if name != 'token_type_ids']
        
        print(f"Tokenizer model_input_names: {tokenizer.model_input_names}")
        
        # æµ‹è¯•å•ä¸ªæ ·æœ¬åˆ†è¯
        sample_text = dataset[0]['text']
        print(f"\nåŸå§‹æ–‡æœ¬ç±»å‹: {type(sample_text)}")
        print(f"åŸå§‹æ–‡æœ¬å†…å®¹: {repr(sample_text[:50])}...")
        
        # åˆ†è¯æµ‹è¯•
        tokens = tokenizer(
            sample_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        print(f"åˆ†è¯ç»“æœ: {list(tokens.keys())}")
        for key, value in tokens.items():
            print(f"  {key}: {value.shape}, {value.dtype}")
        
        # æµ‹è¯•æ‰¹é‡åˆ†è¯
        batch_texts = [item['text'] for item in dataset]
        print(f"\næ‰¹é‡æ–‡æœ¬æ•°é‡: {len(batch_texts)}")
        print(f"æ‰¹é‡æ–‡æœ¬ç±»å‹: {[type(t) for t in batch_texts[:3]]}")
        
        batch_tokens = tokenizer(
            batch_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        print(f"æ‰¹é‡åˆ†è¯ç»“æœ: {list(batch_tokens.keys())}")
        for key, value in batch_tokens.items():
            print(f"  {key}: {value.shape}, {value.dtype}")
        
        # æµ‹è¯•æ•°æ®é›†æ˜ å°„
        def tokenize_function(examples):
            return tokenizer(
                examples['text'],
                padding=False,  # ä¸åœ¨è¿™é‡Œpadding
                truncation=True,
                max_length=512
            )
        
        print(f"\n=== æµ‹è¯•æ•°æ®é›†æ˜ å°„ ===")
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        print(f"æ˜ å°„åæ•°æ®é›†å­—æ®µ: {list(tokenized_dataset[0].keys())}")
        sample_tokenized = tokenized_dataset[0]
        for key, value in sample_tokenized.items():
            print(f"  {key}: {type(value)}, é•¿åº¦: {len(value) if hasattr(value, '__len__') else 'N/A'}")
        
        print("âœ… Tokenizer ä¸æ•°æ®é›†äº¤äº’æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_collator_with_dataset():
    """æµ‹è¯•æ•°æ®æ•´ç†å™¨ä¸æ•°æ®é›†"""
    print(f"\n=== æµ‹è¯•æ•°æ®æ•´ç†å™¨ä¸æ•°æ®é›† ===")
    
    try:
        from transformers import DataCollatorForLanguageModeling
        
        # åˆ›å»ºæ•°æ®é›†å’Œtokenizer
        dataset = create_mock_dataset()
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç§»é™¤ token_type_ids
        if hasattr(tokenizer, 'model_input_names'):
            if 'token_type_ids' in tokenizer.model_input_names:
                tokenizer.model_input_names = [name for name in tokenizer.model_input_names if name != 'token_type_ids']
        
        # åˆ†è¯æ•°æ®é›†
        def tokenize_function(examples):
            return tokenizer(
                examples['text'],
                padding=False,
                truncation=True,
                max_length=512
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )
        
        # æµ‹è¯•æ‰¹é‡æ•´ç†
        batch_size = 2
        batch = [tokenized_dataset[i] for i in range(batch_size)]
        
        print(f"æ‰¹é‡æ•°æ®æ ·æœ¬æ•°: {len(batch)}")
        print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬å­—æ®µ: {list(batch[0].keys())}")
        
        collated = data_collator(batch)
        print(f"æ•´ç†åå­—æ®µ: {list(collated.keys())}")
        for key, value in collated.items():
            print(f"  {key}: {value.shape}, {value.dtype}")
        
        print("âœ… æ•°æ®æ•´ç†å™¨æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ•´ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹ SFT ç‰¹å®šæµ‹è¯•...")
    
    success1 = test_tokenizer_with_dataset()
    success2 = test_data_collator_with_dataset()
    
    if success1 and success2:
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    else:
        print(f"\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥!")