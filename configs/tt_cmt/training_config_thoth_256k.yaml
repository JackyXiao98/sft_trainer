# SFT Scaling Law 训练配置文件 (针对短文本场景优化)

# 模型配置
model:
  model_name: "/mnt/hdfs/selection/tt_stage2_model/v3_domain_cpt/ckpt256k_safe" 
  max_seq_length: 128
  max_seq_length_eval: 128
  use_flash_attention_2: true
  attn_implementation: "flash_attention_2"  # 明确指定注意力实现
  trust_remote_code: true
  use_cache: false  # 训练时禁用缓存以提升效率

# 训练参数
training:
  output_dir: "/home/tiger/.cache/outputs"
  # [优化] 显著增大单设备批次大小，因为序列很短，可以充分利用GPU
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 32
  # [优化] 相应地减少梯度累积步骤，保持或适当增大有效批次大小 (64 * 2 = 128)
  # 直接使用更大的 per_device_train_batch_size 通常比高梯度累积更高效
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  weight_decay: 0.01
  num_train_epochs: 2
  max_steps: -1
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 2
  load_best_model_at_end: false
  metric_for_best_model: "loss"
  greater_is_better: false
  # 注意：当前配置不进行评估和保存，如果需要，请将 "no" 修改为 "steps"
  evaluation_strategy: "no"
  save_strategy: "no"
  report_to: []  # 禁用 wandb 上传，避免长时间等待
  run_name: null  # 将在训练脚本中动态设置
  seed: 42
  data_seed: 42
  # [优化] 可以尝试适当增加数据加载器的工作进程数，以确保数据预处理不会成为瓶颈
  dataloader_num_workers: 8
  remove_unused_columns: true
  label_names: ["labels"]
  
# FSDP配置
fsdp:
  # [优化] 从 full_shard 改为 shard_grad_op，减少通信开销，对1B模型通常更快
  fsdp: "shard_grad_op auto_wrap"
  fsdp_transformer_layer_cls_to_wrap: "ThothDecoderLayer"
  fsdp_backward_prefetch: "backward_pre"
  fsdp_forward_prefetch: false
  fsdp_use_orig_params: true
  fsdp_cpu_ram_efficient_loading: true
  fsdp_auto_wrap_policy: "TRANSFORMER_BASED_WRAP"
  # [优化] FSDP分片策略改为 SHARD_GRAD_OP
  fsdp_sharding_strategy: "SHARD_GRAD_OP"
  fsdp_state_dict_type: "SHARDED_STATE_DICT"
  # [优化] 关闭activation_checkpointing。这是一个以计算换内存的功能。
  # 对于1B模型和短序列，内存通常足够，关闭它可以直接加速训练。
  activation_checkpointing: false
  transformer_layer_cls_to_wrap: "ThothDecoderLayer"

# 数据配置
data:
  # TODO: 请确保将这里的数据集名称更换为您实际的TikTok评论数据集
  dataset_name: "path/to/your/tiktok/dataset"
  dataset_config_names: null # 根据你的数据集进行调整
  token_limits:
    train_base: 660000
    validation: 1000000
  
# Wandb配置
wandb:
  project: "sft-scaling-law"
  entity: null
  tags: ["sft", "scaling-law", "qwen", "tiktok-comments"]
  notes: "SFT Scaling Law实验 - 优化版"
  # 离线模式：设置为 true 时，wandb 只在本地记录，不上传到云端
  offline: true

# SFT特定配置
sft:
  max_seq_length: 128
  # [优化] 开启packing！这是针对短文本最重要的性能优化。
  # 它会将多个短样本拼接成一个长样本，极大提升GPU利用率。
  packing: true
  dataset_text_field: "text"
  dataset_kwargs:
    add_special_tokens: false
    skip_prepare_dataset: true

# 其他配置
misc:
  bf16: true
  fp16: false
  tf32: true
  # [优化] 确保这里也为false，与FSDP配置中的activation_checkpointing保持一致
  gradient_checkpointing: false
  ddp_find_unused_parameters: false
  # [优化] 开启group_by_length，它会将被padding到相似长度的样本组合在一起，进一步减少padding开销
  group_by_length: true
  length_column_name: "length"
  disable_tqdm: false
  prediction_loss_only: true
  include_inputs_for_metrics: false
