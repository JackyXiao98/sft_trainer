#!/usr/bin/env python3
"""
è°ƒè¯•å¼ é‡è½¬æ¢é—®é¢˜çš„ç®€å•è„šæœ¬
"""

import torch
from transformers import AutoTokenizer

def test_tokenization(model_name="microsoft/DialoGPT-medium"):
    """æµ‹è¯•åˆ†è¯è¿‡ç¨‹"""
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    
    # ä½ çš„æ•°æ®æ ·æœ¬
    sample_text = '<|im_start|>user\n<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nğŸ’•<|im_end|>\n'
    
    print(f"åŸå§‹æ–‡æœ¬: {repr(sample_text)}")
    print(f"æ–‡æœ¬ç±»å‹: {type(sample_text)}")
    print(f"æ–‡æœ¬é•¿åº¦: {len(sample_text)}")
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # è®¾ç½®pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f"è®¾ç½® pad_token: {tokenizer.pad_token}")
        
        # ç§»é™¤token_type_idsæ”¯æŒ
        if hasattr(tokenizer, 'model_input_names'):
            if 'token_type_ids' in tokenizer.model_input_names:
                tokenizer.model_input_names = [name for name in tokenizer.model_input_names if name != 'token_type_ids']
                print("ç§»é™¤äº† token_type_ids æ”¯æŒ")
        
        print(f"Tokenizer model_input_names: {tokenizer.model_input_names}")
        
        # æµ‹è¯•åˆ†è¯
        print("\n=== æµ‹è¯•åˆ†è¯ ===")
        
        # æ–¹æ³•1: åŸºæœ¬åˆ†è¯
        tokens = tokenizer(sample_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        print(f"åˆ†è¯ç»“æœé”®: {list(tokens.keys())}")
        for key, value in tokens.items():
            print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
        
        # æ–¹æ³•2: æ‰¹é‡åˆ†è¯
        batch_texts = [sample_text, sample_text]  # æ¨¡æ‹Ÿæ‰¹é‡æ•°æ®
        batch_tokens = tokenizer(batch_texts, return_tensors="pt", padding=True, truncation=True, max_length=512)
        print(f"\næ‰¹é‡åˆ†è¯ç»“æœé”®: {list(batch_tokens.keys())}")
        for key, value in batch_tokens.items():
            print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
        
        print("\nâœ… åˆ†è¯æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"\nâŒ åˆ†è¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_data_collator():
    """æµ‹è¯•æ•°æ®æ•´ç†å™¨"""
    print("\n=== æµ‹è¯•æ•°æ®æ•´ç†å™¨ ===")
    
    try:
        from transformers import DataCollatorForLanguageModeling, AutoTokenizer
        
        # ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„tokenizerè¿›è¡Œæµ‹è¯•
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç§»é™¤token_type_ids
        if hasattr(tokenizer, 'model_input_names'):
            if 'token_type_ids' in tokenizer.model_input_names:
                tokenizer.model_input_names = [name for name in tokenizer.model_input_names if name != 'token_type_ids']
        
        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
        
        # æ¨¡æ‹Ÿæ•°æ®
        sample_text = '<|im_start|>user\n<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\nğŸ’•<|im_end|>\n'
        
        # åˆ†è¯
        tokenized = tokenizer(sample_text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆæ¨¡æ‹Ÿæ•°æ®é›†æ ¼å¼ï¼‰
        batch = []
        for i in range(len(tokenized['input_ids'])):
            item = {}
            for key, value in tokenized.items():
                item[key] = value[i]
            batch.append(item)
        
        # æµ‹è¯•æ•°æ®æ•´ç†å™¨
        collated = data_collator(batch)
        print(f"æ•´ç†åçš„æ•°æ®é”®: {list(collated.keys())}")
        for key, value in collated.items():
            print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
        
        print("âœ… æ•°æ®æ•´ç†å™¨æµ‹è¯•æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ•´ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹è°ƒè¯•å¼ é‡è½¬æ¢é—®é¢˜...")
    
    # æµ‹è¯•ä¸åŒçš„tokenizer
    models_to_test = [
        "gpt2",  # åŸºç¡€æµ‹è¯•
        "microsoft/DialoGPT-medium",  # å¯¹è¯æ¨¡å‹
    ]
    
    for model in models_to_test:
        print(f"\n{'='*50}")
        try:
            success = test_tokenization(model)
            if not success:
                print(f"æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"æ— æ³•æµ‹è¯•æ¨¡å‹ {model}: {e}")
    
    # æµ‹è¯•æ•°æ®æ•´ç†å™¨
    test_data_collator()
    
    print(f"\n{'='*50}")
    print("è°ƒè¯•å®Œæˆ!")