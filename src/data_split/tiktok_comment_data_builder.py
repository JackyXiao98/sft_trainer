#!/usr/bin/env python3
"""
TikTokè¯„è®ºæ•°æ®é›†æ„å»ºå™¨
ç”¨äºå¤„ç†/mnt/hdfs/selection/tiktok_cmtä¸‹çš„10ä¸ªåŒºé—´æ•°æ®é›†ï¼Œå°†docå­—æ®µè½¬æ¢ä¸ºSFTè®­ç»ƒæ ¼å¼
"""

import os
import glob
import random
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datasets import load_dataset, Dataset, concatenate_datasets
from rich.console import Console
from rich.progress import Progress

# å¯¼å…¥åŸºç¡€DataBuilderç±»
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data_builder import DataBuilder

console = Console()

class TikTokCommentDataBuilder(DataBuilder):
    """å¤„ç†TikTokè¯„è®ºæ•°æ®é›†çš„DataBuilderå­ç±»"""
    
    def __init__(self, config_path: str = "configs/training_config.yaml"):
        super().__init__(config_path)
        
        # TikTokè¯„è®ºæ•°æ®é›†é…ç½®
        self.base_dir = "/mnt/hdfs/selection/tiktok_cmt"
        
        # 10ä¸ªåŒºé—´æ•°æ®é›†
        self.interval_datasets = [
            "[0.0, 0.1)",
            "[0.1, 0.2)",
            "[0.2, 0.3)",
            "[0.3, 0.4)",
            "[0.4, 0.5)",
            "[0.5, 0.6)",
            "[0.6, 0.7)",
            "[0.7, 0.8)",
            "[0.8, 0.9)",
            "[0.9, 1.0)"
        ]
        
        console.print(f"[green]å‘ç° {len(self.interval_datasets)} ä¸ªTikTokè¯„è®ºåŒºé—´æ•°æ®é›†[/green]")
        console.print(f"å°†ç”Ÿæˆ {1 + len(self.interval_datasets) * 4} ä¸ªè®­ç»ƒé›†")
    
    def convert_doc_to_sft_format(self, doc: str, comment_id: str = None) -> List[Dict]:
        """å°†docå­—æ®µè½¬æ¢ä¸ºSFTè®­ç»ƒæ ¼å¼çš„messages"""
        if not doc or not isinstance(doc, str):
            return []
        
        # åˆ›å»ºç®€å•çš„SFTæ ¼å¼ï¼šç”¨æˆ·æä¾›è¯„è®ºå†…å®¹ï¼ŒåŠ©æ‰‹å›å¤åˆ†æ
        messages = [
            {
                "role": "user",
                "content": ""
            },
            {
                "role": "assistant", 
                "content": doc
            }
        ]
        
        return messages
    
    def load_parquet_files_from_directory(self, directory_path: str) -> Dataset:
        """ä»ç›®å½•ä¸­åŠ è½½æ‰€æœ‰parquetæ–‡ä»¶å¹¶åˆå¹¶"""
        if not os.path.exists(directory_path):
            console.print(f"[red]ç›®å½•ä¸å­˜åœ¨: {directory_path}[/red]")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰parquetæ–‡ä»¶
        escaped_path = glob.escape(directory_path)
        parquet_files = glob.glob(os.path.join(escaped_path, "*.parquet"))

        if not parquet_files:
            console.print(f"[yellow]ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°parquetæ–‡ä»¶: {directory_path}[/yellow]")
            return None
        
        console.print(f"  ğŸ“ æ‰¾åˆ° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
        
        datasets_to_concat = []
        
        try:
            # ç›´æ¥ä½¿ç”¨data_filesåŠ è½½æ‰€æœ‰parquetæ–‡ä»¶
            console.print(f"    ğŸ“‚ æ­£åœ¨åŠ è½½ {len(parquet_files)} ä¸ªparquetæ–‡ä»¶...")
            dataset_dict = load_dataset('parquet', data_files=parquet_files)
            combined_dataset = dataset_dict['train']
            console.print(f"    âœ… åŠ è½½æˆåŠŸ: {len(combined_dataset):,} æ€»æ ·æœ¬")
        except Exception as e:
            console.print(f"    âŒ åŠ è½½å¤±è´¥: {e}")
            return None

        return combined_dataset
    
    def _process_single_example(self, example: Dict) -> Dict:
        """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œå°†docå­—æ®µè½¬æ¢ä¸ºSFTæ ¼å¼"""
        # è·å–docå­—æ®µ
        doc = example.get('doc', '')
        comment_id = example.get('comment_id', '')
        
        if not doc:
            return None
        
        # è½¬æ¢ä¸ºSFTæ ¼å¼
        messages = self.convert_doc_to_sft_format(doc, comment_id)
        
        if not messages:
            return None
        
        # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
        text = self.convert_messages_to_text(messages)
        
        if not text:
            return None
        
        # åˆ›å»ºæ–°çš„example
        new_example = {
            'text': text,
            'messages': messages,
            'original_comment_id': comment_id,
            'interval': example.get('interval', ''),
            'dataset': example.get('dataset', ''),
            'category': example.get('category', ''),
        }
        
        # ä¿ç•™å…¶ä»–æœ‰ç”¨çš„å­—æ®µ
        for key in ['language', 'language_score', 'token_num']:
            if key in example:
                new_example[key] = example[key]
        
        return new_example

    def process_tiktok_dataset(self, dataset: Dataset, dataset_name: str) -> Dataset:
        """å¤„ç†TikTokæ•°æ®é›†ï¼Œå°†docå­—æ®µè½¬æ¢ä¸ºSFTæ ¼å¼ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
        console.print(f"[yellow]æ­£åœ¨å¤„ç† {dataset_name} æ•°æ®é›†...[/yellow]")
        
        processed_examples = []
        total_samples = len(dataset)
        
        # ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†
        max_workers = min(8, os.cpu_count() or 4)  # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°
        console.print(f"[blue]ä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†...[/blue]")
        
        with Progress() as progress:
            task = progress.add_task(f"å¤„ç† {dataset_name}", total=total_samples)
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {
                    executor.submit(self._process_single_example, example): i 
                    for i, example in enumerate(dataset)
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_index):
                    try:
                        result = future.result()
                        if result is not None:
                            processed_examples.append(result)
                        
                        progress.update(task, advance=1)
                        
                        # æ¯1000ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡æè¿°
                        if len(processed_examples) % 1000 == 0:
                            progress.update(task, description=f"å¤„ç† {dataset_name} (å·²å¤„ç†: {len(processed_examples):,} æ ·æœ¬)")
                            
                    except Exception as e:
                        console.print(f"[red]å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}[/red]")
                        progress.update(task, advance=1)
                        continue
        
        console.print(f"[green]{dataset_name} å¤„ç†å®Œæˆ: {len(processed_examples):,} æ ·æœ¬[/green]")
        
        # åˆ›å»ºæ–°çš„Dataset
        processed_dataset = Dataset.from_list(processed_examples)
        return processed_dataset
    
    def load_source_datasets(self) -> Dict[str, Dataset]:
        """åŠ è½½TikTokè¯„è®ºæºæ•°æ®é›†"""
        console.print("[blue]æ­£åœ¨åŠ è½½TikTokè¯„è®ºæ•°æ®é›†...[/blue]")
        
        datasets = {}
        
        for interval in self.interval_datasets:
            console.print(f"\n[cyan]å¤„ç†åŒºé—´: {interval}[/cyan]")
            
            # æ„å»ºè®­ç»ƒæ•°æ®è·¯å¾„
            train_dir = os.path.join(self.base_dir, interval, "train")
            
            # åŠ è½½è®­ç»ƒæ•°æ®
            train_dataset = self.load_parquet_files_from_directory(train_dir)
            
            if train_dataset is None:
                console.print(f"  âŒ æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®: {interval}")
                continue
            
            # å¤„ç†æ•°æ®é›†ï¼Œè½¬æ¢ä¸ºSFTæ ¼å¼
            processed_dataset = self.process_tiktok_dataset(train_dataset, f"{interval}_train")
            
            if processed_dataset is None or len(processed_dataset) == 0:
                console.print(f"  âŒ å¤„ç†åæ•°æ®é›†ä¸ºç©º: {interval}")
                continue
            
            # å­˜å‚¨æ•°æ®é›†
            datasets[interval] = processed_dataset
            console.print(f"  âœ… æˆåŠŸå¤„ç†åŒºé—´ {interval}: {len(processed_dataset):,} æ ·æœ¬")
        
        console.print(f"\n[green]æˆåŠŸåŠ è½½ {len(datasets)} ä¸ªåŒºé—´æ•°æ®é›†[/green]")
        return datasets
    
    def load_validation_datasets(self) -> Dict[str, Dataset]:
        """åŠ è½½TikTokè¯„è®ºéªŒè¯æ•°æ®é›†"""
        console.print("[blue]æ­£åœ¨åŠ è½½TikTokè¯„è®ºéªŒè¯æ•°æ®é›†...[/blue]")
        
        validation_datasets = {}
        
        for interval in self.interval_datasets:
            console.print(f"\n[cyan]å¤„ç†éªŒè¯é›†åŒºé—´: {interval}[/cyan]")
            
            # æ„å»ºéªŒè¯æ•°æ®è·¯å¾„
            val_dir = os.path.join(self.base_dir, interval, "val")
            
            # åŠ è½½éªŒè¯æ•°æ®
            val_dataset = self.load_parquet_files_from_directory(val_dir)
            
            if val_dataset is None:
                console.print(f"  âŒ æ— æ³•åŠ è½½éªŒè¯æ•°æ®: {interval}")
                continue
            
            # å¤„ç†æ•°æ®é›†ï¼Œè½¬æ¢ä¸ºSFTæ ¼å¼
            processed_dataset = self.process_tiktok_dataset(val_dataset, f"{interval}_val")
            
            if processed_dataset is None or len(processed_dataset) == 0:
                console.print(f"  âŒ å¤„ç†åéªŒè¯æ•°æ®é›†ä¸ºç©º: {interval}")
                continue
            
            # æ¸…ç†intervalåç§°ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
            clean_interval = interval.replace("[", "").replace(")", "").replace(", ", "_").replace(".", "")
            
            # å­˜å‚¨éªŒè¯æ•°æ®é›†
            validation_datasets[f"{clean_interval}_val"] = processed_dataset
            console.print(f"  âœ… æˆåŠŸå¤„ç†éªŒè¯é›† {interval}: {len(processed_dataset):,} æ ·æœ¬")
        
        console.print(f"\n[green]æˆåŠŸåŠ è½½ {len(validation_datasets)} ä¸ªéªŒè¯æ•°æ®é›†[/green]")
        return validation_datasets
    
    def create_training_variants(self, base_datasets: Dict[str, Dataset]) -> Dict[str, Dataset]:
        """åˆ›å»ºè®­ç»ƒå˜ä½“ï¼š1ä¸ªå…¨é‡ + 10*4ä¸ªå˜ä½“æ‰°åŠ¨"""
        console.print("[blue]åˆ›å»ºTikTokè¯„è®ºè®­ç»ƒå˜ä½“...[/blue]")
        
        training_datasets = {}
        dataset_names = list(base_datasets.keys())
        
        # 1. åˆ›å»ºå…¨é‡æ•°æ®é›†ï¼ˆæ‰€æœ‰10ä¸ªåŒºé—´æ•°æ®é›†çš„ç»„åˆï¼‰
        console.print("åˆ›å»ºå…¨é‡è®­ç»ƒæ•°æ®é›†...")
        all_datasets = list(base_datasets.values())
        if all_datasets:
            full_dataset = concatenate_datasets(all_datasets)
            training_datasets["tiktok_full_dataset"] = full_dataset
            console.print(f"[green]å…¨é‡æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(full_dataset):,} æ ·æœ¬[/green]")
        
        # 2. ä¸ºæ¯ä¸ªåŒºé—´æ•°æ®é›†åˆ›å»º4ç§å˜ä½“æ‰°åŠ¨ï¼ˆ1/3, 1/2, 2x, 3xï¼‰+ æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        console.print(f"\nå¼€å§‹ä¸º {len(dataset_names)} ä¸ªåŒºé—´æ•°æ®é›†åˆ›å»ºå˜ä½“æ‰°åŠ¨...")
        
        for i, target_interval in enumerate(dataset_names, 1):
            console.print(f"\n[cyan]({i}/{len(dataset_names)}) ä¸º {target_interval} åˆ›å»º4ç§å˜ä½“æ‰°åŠ¨[/cyan]")
            
            target_dataset = base_datasets[target_interval]
            
            # è·å–æ‰€æœ‰å…¶ä»–æ•°æ®é›†
            other_datasets = [base_datasets[name] for name in dataset_names if name != target_interval]
            
            # åˆå¹¶æ‰€æœ‰å…¶ä»–æ•°æ®é›†
            if other_datasets:
                other_combined = concatenate_datasets(other_datasets)
            else:
                # å¦‚æœæ²¡æœ‰å…¶ä»–æ•°æ®é›†ï¼Œä½¿ç”¨ç©ºæ•°æ®é›†
                other_combined = Dataset.from_dict({})
            
            # åˆ›å»º4ç§å˜ä½“æ‰°åŠ¨
            variants = self._create_interval_variants(target_dataset, other_combined, target_interval)
            training_datasets.update(variants)
        
        console.print(f"\n[green]åˆ›å»ºäº† {len(training_datasets)} ä¸ªè®­ç»ƒæ•°æ®é›†[/green]")
        return training_datasets
    
    def _create_interval_variants(self, target_dataset: Dataset, other_combined: Dataset, interval_name: str) -> Dict[str, Dataset]:
        """ä¸ºç‰¹å®šåŒºé—´åˆ›å»º4ä¸ªå˜ä½“æ‰°åŠ¨"""
        variants = {}
        
        # æ¸…ç†åŒºé—´åç§°ï¼Œç”¨äºæ–‡ä»¶å
        clean_name = interval_name.replace("[", "").replace(")", "").replace(", ", "_").replace(".", "")
        
        # 1/3 å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„1/3 + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        if len(target_dataset) >= 3:
            subset_1_3 = target_dataset.select(random.sample(range(len(target_dataset)), len(target_dataset) // 3))
            if len(other_combined) > 0:
                variants[f'tiktok_{clean_name}_1_3'] = concatenate_datasets([subset_1_3, other_combined])
            else:
                variants[f'tiktok_{clean_name}_1_3'] = subset_1_3
            console.print(f"  [green]tiktok_{clean_name}_1_3 åˆ›å»ºå®Œæˆ: {len(variants[f'tiktok_{clean_name}_1_3']):,} æ ·æœ¬[/green]")
        
        # 1/2 å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„1/2 + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        if len(target_dataset) >= 2:
            subset_1_2 = target_dataset.select(random.sample(range(len(target_dataset)), len(target_dataset) // 2))
            if len(other_combined) > 0:
                variants[f'tiktok_{clean_name}_1_2'] = concatenate_datasets([subset_1_2, other_combined])
            else:
                variants[f'tiktok_{clean_name}_1_2'] = subset_1_2
            console.print(f"  [green]tiktok_{clean_name}_1_2 åˆ›å»ºå®Œæˆ: {len(variants[f'tiktok_{clean_name}_1_2']):,} æ ·æœ¬[/green]")
        
        # 2x å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„2å€ + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        double_dataset = concatenate_datasets([target_dataset, target_dataset])
        if len(other_combined) > 0:
            variants[f'tiktok_{clean_name}_2x'] = concatenate_datasets([double_dataset, other_combined])
        else:
            variants[f'tiktok_{clean_name}_2x'] = double_dataset
        console.print(f"  [green]tiktok_{clean_name}_2x åˆ›å»ºå®Œæˆ: {len(variants[f'tiktok_{clean_name}_2x']):,} æ ·æœ¬[/green]")
        
        # 3x å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„3å€ + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        triple_dataset = concatenate_datasets([target_dataset, target_dataset, target_dataset])
        if len(other_combined) > 0:
            variants[f'tiktok_{clean_name}_3x'] = concatenate_datasets([triple_dataset, other_combined])
        else:
            variants[f'tiktok_{clean_name}_3x'] = triple_dataset
        console.print(f"  [green]tiktok_{clean_name}_3x åˆ›å»ºå®Œæˆ: {len(variants[f'tiktok_{clean_name}_3x']):,} æ ·æœ¬[/green]")
        
        return variants
    
    def save_datasets(self, training_datasets: Dict[str, Dataset], 
                     validation_datasets: Dict[str, Dataset]):
        """ä¿å­˜æ‰€æœ‰æ•°æ®é›†åˆ°ç£ç›˜"""
        console.print("[blue]ä¿å­˜TikTokè¯„è®ºæ•°æ®é›†åˆ°ç£ç›˜...[/blue]")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs("data/training", exist_ok=True)
        os.makedirs("data/validation", exist_ok=True)
        
        # ä¿å­˜è®­ç»ƒæ•°æ®é›†
        for name, dataset in training_datasets.items():
            save_path = f"data/training/{name}"
            dataset.save_to_disk(save_path)
            console.print(f"[green]è®­ç»ƒæ•°æ®é›† {name} å·²ä¿å­˜åˆ° {save_path}[/green]")
        
        # ä¿å­˜éªŒè¯æ•°æ®é›†
        for name, dataset in validation_datasets.items():
            save_path = f"data/validation/{name}"
            dataset.save_to_disk(save_path)
            console.print(f"[green]éªŒè¯æ•°æ®é›† {name} å·²ä¿å­˜åˆ° {save_path}[/green]")
        
        console.print("[bold green]æ‰€æœ‰TikTokè¯„è®ºæ•°æ®é›†ä¿å­˜å®Œæˆ![/bold green]")
    
    def build_all_datasets(self):
        """æ„å»ºæ‰€æœ‰TikTokè¯„è®ºæ•°æ®é›†"""
        console.print("[bold green]å¼€å§‹æ„å»ºTikTokè¯„è®ºæ•°æ®é›†...[/bold green]")
        
        # 1. åŠ è½½æºæ•°æ®é›†
        source_datasets = self.load_source_datasets()
        if not source_datasets:
            console.print("[red]æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æºæ•°æ®é›†ï¼Œé€€å‡º[/red]")
            return
        
        # 2. åŠ è½½éªŒè¯æ•°æ®é›†
        validation_datasets = self.load_validation_datasets()
        
        # 3. åˆ›å»ºè®­ç»ƒå˜ä½“ï¼ˆä¸éœ€è¦é¢å¤–çš„tokené‡‡æ ·ï¼Œå› ä¸ºæ•°æ®å·²ç»é¢„å¤„ç†è¿‡ï¼‰
        training_datasets = self.create_training_variants(source_datasets)
        
        # 4. ä¿å­˜æ•°æ®é›†
        self.save_datasets(training_datasets, validation_datasets)
        
        console.print("[bold green]TikTokè¯„è®ºæ•°æ®é›†æ„å»ºå®Œæˆï¼[/bold green]")
        console.print(f"è®­ç»ƒæ•°æ®é›†: {len(training_datasets)} ä¸ª")
        console.print(f"éªŒè¯æ•°æ®é›†: {len(validation_datasets)} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="TikTokè¯„è®ºæ•°æ®é›†æ„å»ºå™¨")
    parser.add_argument("--config", type=str, default="configs/training_config.yaml", 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    console.print("[bold blue]TikTokè¯„è®ºæ•°æ®é›†æ„å»ºå™¨[/bold blue]")
    console.print(f"[blue]ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}[/blue]")
    
    builder = TikTokCommentDataBuilder(args.config)
    builder.build_all_datasets()


if __name__ == "__main__":
    main()