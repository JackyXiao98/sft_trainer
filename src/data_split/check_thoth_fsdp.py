#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import os
import torch
import argparse
from transformers import AutoModelForCausalLM, AutoConfig

def main():
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="æ£€æŸ¥æ¨¡å‹çš„ FSDP Transformer Layer ç±»å")
    parser.add_argument("--model_path", type=str, 
                       default="/mnt/hdfs/selection/tt_stage2_model/domain",
                       help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--trust_remote_code", action="store_true", 
                       help="æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆç”¨äºè‡ªå®šä¹‰æ¨¡å‹ï¼‰")
    
    args = parser.parse_args()
    model_path = args.model_path

    print(f"--- æ­¥éª¤ 1: æ£€æŸ¥æ¨¡å‹è·¯å¾„ ---")
    print(f"æ¨¡å‹: {model_path}")

    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
    is_local_path = os.path.isdir(model_path)
    if is_local_path:
        print("âœ“ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„")
    else:
        print("âœ“ æ£€æµ‹åˆ° Hugging Face æ¨¡å‹åç§°ï¼Œå°†ä»è¿œç¨‹ä¸‹è½½")
    
    print("æ¨¡å‹è·¯å¾„æ£€æŸ¥é€šè¿‡ï¼")

    # --- 2. å°è¯•åŠ è½½é…ç½®æ–‡ä»¶ ---
    try:
        print("\n--- æ­¥éª¤ 2: åŠ è½½æ¨¡å‹é…ç½® ---")
        config = AutoConfig.from_pretrained(
            model_path, 
            trust_remote_code=args.trust_remote_code
        )
        print(f"æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"æ¶æ„: {config.architectures if hasattr(config, 'architectures') else 'æœªçŸ¥'}")
        print("é…ç½®åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åŠ è½½é…ç½®æ—¶å‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹ç›®å½•ä¸­æ˜¯å¦å­˜åœ¨ 'config.json' æ–‡ä»¶ã€‚")
        sys.exit(1)

    # --- 3. åŠ è½½æ¨¡å‹ ---
    try:
        print(f"\n--- æ­¥éª¤ 3: ä» '{model_path}' åŠ è½½æ¨¡å‹ ---")
        print("æ³¨æ„: ä»…åŠ è½½æ¨¡å‹ç»“æ„ï¼Œä¸åŠ è½½æƒé‡åˆ°GPUä»¥èŠ‚çœå†…å­˜...")
        
        # ä½¿ç”¨ AutoModelForCausalLM è‡ªåŠ¨è¯†åˆ«æ¨¡å‹ç±»å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            low_cpu_mem_usage=True,
            dtype=torch.bfloat16,
            trust_remote_code=args.trust_remote_code,
            device_map="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…GPUå†…å­˜é—®é¢˜
        )
        print("æ¨¡å‹ç»“æ„åŠ è½½æˆåŠŸï¼")
        print(f"æ¨¡å‹ç±»: {model.__class__.__name__}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ï¼Œä»¥åŠæƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
        print("å¦‚æœæ˜¯è‡ªå®šä¹‰æ¨¡å‹ï¼Œè¯·å°è¯•æ·»åŠ  --trust_remote_code å‚æ•°ã€‚")
        sys.exit(1)


    # --- 4. æŸ¥æ‰¾ Transformer Block ç±»å ---
    print("\n--- æ­¥éª¤ 4: æŸ¥æ‰¾ FSDP Transformer Block ç±»å ---")

    def find_transformer_block_class(model):
        """
        é€’å½’æŸ¥æ‰¾æ¨¡å‹ä¸­çš„ Transformer Block ç±»å
        """
        # å¸¸è§çš„ Transformer Block ç±»åæ¨¡å¼
        common_patterns = [
            'TransformerBlock', 'Block', 'Layer', 'DecoderLayer', 
            'EncoderLayer', 'ThothBlock', 'ThothLayer', 'QWenBlock',
            'LlamaDecoderLayer', 'GPTBlock', 'AttentionBlock'
        ]
        
        def search_modules(module, path="", depth=0):
             results = []
             for name, child in module.named_children():
                 current_path = f"{path}.{name}" if path else name
                 class_name = child.__class__.__name__
                 
                 # æ£€æŸ¥æ˜¯å¦åŒ¹é…å¸¸è§æ¨¡å¼
                 for pattern in common_patterns:
                     if pattern.lower() in class_name.lower():
                         # åªæ·»åŠ ä¸»è¦çš„ transformer blockï¼Œé¿å…å­æ¨¡å—
                         if depth <= 2 and any(main_pattern in class_name for main_pattern in 
                                             ['Block', 'Layer', 'DecoderLayer', 'EncoderLayer']):
                             results.append((current_path, class_name, child))
                         break
                 
                 # é€’å½’æœç´¢å­æ¨¡å—ï¼Œä½†é™åˆ¶æ·±åº¦
                 if depth < 3:
                     results.extend(search_modules(child, current_path, depth + 1))
             
             return results
        
        return search_modules(model)

    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ Transformer Block
    transformer_blocks = find_transformer_block_class(model)

    if transformer_blocks:
         # è·å–å”¯ä¸€çš„ç±»å
         unique_classes = {}
         for path, class_name, module in transformer_blocks:
             if class_name not in unique_classes:
                 unique_classes[class_name] = (path, module)
         
         print("ğŸ‰ æ‰¾åˆ°ä»¥ä¸‹å¯èƒ½çš„ Transformer Block ç±»:")
         for i, (class_name, (example_path, module)) in enumerate(unique_classes.items(), 1):
             print(f"  {i}. ç±»å: {class_name}")
             print(f"     ç¤ºä¾‹è·¯å¾„: {example_path}")
             print(f"     æ¨¡å—ç±»å‹: {type(module).__name__}")
             print()
         
         # æ‰¾åˆ°æœ€å¯èƒ½çš„ transformer blockï¼ˆé€šå¸¸åŒ…å« 'Block' æˆ– 'Layer'ï¼‰
         main_transformer_classes = [name for name in unique_classes.keys() 
                                   if any(pattern in name for pattern in ['Block', 'DecoderLayer', 'EncoderLayer'])]
         
         if main_transformer_classes:
             recommended_class = main_transformer_classes[0]
             recommended_path = unique_classes[recommended_class][0]
         else:
             # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦çš„ï¼Œå°±ç”¨ç¬¬ä¸€ä¸ª
             recommended_class = list(unique_classes.keys())[0]
             recommended_path = unique_classes[recommended_class][0]
         
         print(f"ğŸ”§ æ¨èç”¨äº FSDP çš„ Transformer Block ç±»å: {recommended_class}")
         print(f"ğŸ”§ æ¨èçš„æ¨¡å—è·¯å¾„: {recommended_path}")
         
         # è¾“å‡º FSDP é…ç½®å»ºè®®
         print(f"\nğŸ“‹ FSDP é…ç½®å»ºè®®:")
         print(f"   auto_wrap_policy: transformer_auto_wrap_policy")
         print(f"   transformer_layer_cls_to_wrap: {recommended_class}")
        
    else:
        print("âŒ æœªæ‰¾åˆ°æ˜æ˜¾çš„ Transformer Block ç±»ã€‚")
        print("è®©æˆ‘ä»¬æŸ¥çœ‹æ¨¡å‹çš„æ•´ä½“ç»“æ„:")
        print("\næ¨¡å‹ç»“æ„æ¦‚è§ˆ:")
        for name, module in model.named_children():
            print(f"  - {name}: {module.__class__.__name__}")
        
        print("\nè¯·æ‰‹åŠ¨æ£€æŸ¥æ¨¡å‹ç»“æ„ä»¥ç¡®å®šæ­£ç¡®çš„ layer ç±»åã€‚")

    print("\n--- å®Œæˆ! ---")
    print("ç°åœ¨ä½ å¯ä»¥åœ¨ FSDP é…ç½®ä¸­ä½¿ç”¨æ‰¾åˆ°çš„ç±»åã€‚")

if __name__ == "__main__":
    main()