#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å°† PyTorch æ¨¡å‹æ–‡ä»¶è½¬æ¢ä¸º safetensors æ ¼å¼
ç”¨äºè§£å†³ PyTorch å®‰å…¨æ¼æ´é—®é¢˜ (CVE-2025-32434)
"""

import os
import sys
import argparse
import json
import shutil
from pathlib import Path
from typing import Dict, Any, Optional

try:
    import torch
    from safetensors.torch import save_file, load_file, save_model, load_model
    from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…å¿…è¦çš„ä¾èµ–:")
    print("pip install torch safetensors transformers")
    sys.exit(1)


def detect_shared_tensors(tensors: Dict[str, torch.Tensor]) -> Dict[str, list]:
    """æ£€æµ‹å…±äº«å¼ é‡"""
    shared_tensors = {}
    tensor_to_names = {}
    
    for name, tensor in tensors.items():
        tensor_id = id(tensor.storage())
        if tensor_id not in tensor_to_names:
            tensor_to_names[tensor_id] = []
        tensor_to_names[tensor_id].append(name)
    
    for tensor_id, names in tensor_to_names.items():
        if len(names) > 1:
            shared_tensors[tensor_id] = names
    
    return shared_tensors


def resolve_shared_tensors(tensors: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
    """è§£å†³å…±äº«å¼ é‡é—®é¢˜ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªå¼ é‡ï¼Œå…¶ä»–çš„åˆ›å»ºå‰¯æœ¬"""
    shared_tensors = detect_shared_tensors(tensors)
    resolved_tensors = {}
    
    if shared_tensors:
        print(f"âš ï¸  æ£€æµ‹åˆ°å…±äº«å¼ é‡: {list(shared_tensors.values())}")
        print("æ­£åœ¨åˆ›å»ºç‹¬ç«‹å‰¯æœ¬ä»¥é¿å…å…±äº«å†…å­˜é—®é¢˜...")
    
    processed_storages = set()
    
    for name, tensor in tensors.items():
        tensor_id = id(tensor.storage())
        
        if tensor_id in processed_storages:
            # ä¸ºå…±äº«å¼ é‡åˆ›å»ºç‹¬ç«‹å‰¯æœ¬
            resolved_tensors[name] = tensor.clone().detach()
            print(f"  - ä¸º {name} åˆ›å»ºç‹¬ç«‹å‰¯æœ¬")
        else:
            # ç¬¬ä¸€æ¬¡é‡åˆ°è¿™ä¸ªå­˜å‚¨ï¼Œç›´æ¥ä½¿ç”¨
            resolved_tensors[name] = tensor
            processed_storages.add(tensor_id)
    
    return resolved_tensors


def convert_pytorch_to_safetensors(
    model_path: str,
    output_path: str,
    trust_remote_code: bool = False,
    max_shard_size: str = "5GB",
    use_model_api: bool = True
) -> None:
    """
    å°† PyTorch æ¨¡å‹è½¬æ¢ä¸º safetensors æ ¼å¼
    
    Args:
        model_path: è¾“å…¥æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        trust_remote_code: æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
        max_shard_size: æœ€å¤§åˆ†ç‰‡å¤§å°
        use_model_api: æ˜¯å¦ä½¿ç”¨ save_model API (æ¨èï¼Œè‡ªåŠ¨å¤„ç†å…±äº«å¼ é‡)
    """
    print(f"ğŸ”„ å¼€å§‹è½¬æ¢æ¨¡å‹: {model_path}")
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_path, exist_ok=True)
    
    # æ–¹æ³•1: å°è¯•ä½¿ç”¨ save_model API (æ¨èæ–¹æ³•)
    if use_model_api:
        try:
            print("\n--- æ–¹æ³•1: ä½¿ç”¨ save_model API (æ¨è) ---")
            print("æ­£åœ¨åŠ è½½å®Œæ•´æ¨¡å‹...")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„
            if os.path.isdir(model_path):
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=trust_remote_code,
                    dtype=torch.float16,
                    device_map="cpu"
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    trust_remote_code=trust_remote_code,
                    dtype=torch.float16,
                    device_map="cpu"
                )
            
            print("æ­£åœ¨ä¿å­˜ä¸º safetensors æ ¼å¼...")
            output_file = os.path.join(output_path, "model.safetensors")
            save_model(model, output_file)
            print(f"âœ“ ä¿å­˜å®Œæˆ: model.safetensors")
            
            # å¤åˆ¶é…ç½®æ–‡ä»¶
            print("\n--- å¤åˆ¶é…ç½®æ–‡ä»¶ ---")
            config_files = [
                "config.json", "tokenizer.json", "tokenizer_config.json", 
                "special_tokens_map.json", "vocab.txt", "vocab.json", "merges.txt"
            ]
            
            for config_file in config_files:
                src_file = os.path.join(model_path, config_file)
                dst_file = os.path.join(output_path, config_file)
                
                if os.path.exists(src_file):
                    shutil.copy2(src_file, dst_file)
                    print(f"âœ“ å¤åˆ¶: {config_file}")
            
            print(f"\nğŸ‰ è½¬æ¢å®Œæˆ! è¾“å‡ºç›®å½•: {output_path}")
            print("ä½¿ç”¨ save_model API æˆåŠŸå¤„ç†äº†å…±äº«å¼ é‡é—®é¢˜ã€‚")
            return
            
        except Exception as e:
            print(f"âš ï¸  save_model API å¤±è´¥: {e}")
            print("å›é€€åˆ°æ‰‹åŠ¨å¤„ç†æ–¹æ³•...")
            use_model_api = False
    
    # æ–¹æ³•2: æ‰‹åŠ¨å¤„ç†æƒé‡æ–‡ä»¶
    print("\n--- æ–¹æ³•2: æ‰‹åŠ¨å¤„ç†æƒé‡æ–‡ä»¶ ---")
    
    # 1. å¤åˆ¶é…ç½®æ–‡ä»¶
    print("\n--- æ­¥éª¤ 1: å¤åˆ¶é…ç½®æ–‡ä»¶ ---")
    config_files = [
        "config.json", "tokenizer.json", "tokenizer_config.json", 
        "special_tokens_map.json", "vocab.txt", "vocab.json", "merges.txt",
        "mariana_config.json", "configuration_thoth.py", "modeling_thoth.py"
    ]
    
    for config_file in config_files:
        src_file = os.path.join(model_path, config_file)
        dst_file = os.path.join(output_path, config_file)
        
        if os.path.exists(src_file):
            shutil.copy2(src_file, dst_file)
            print(f"âœ“ å¤åˆ¶: {config_file}")
    
    # 2. æŸ¥æ‰¾æ‰€æœ‰ PyTorch æƒé‡æ–‡ä»¶
    print("\n--- æ­¥éª¤ 2: æŸ¥æ‰¾ PyTorch æƒé‡æ–‡ä»¶ ---")
    pytorch_files = []
    
    # æŸ¥æ‰¾ä¸åŒæ ¼å¼çš„æƒé‡æ–‡ä»¶
    weight_patterns = [
        "pytorch_model.bin",
        "pytorch_model-*.bin", 
        "model.safetensors",
        "model-*.safetensors"
    ]
    
    for pattern in weight_patterns:
        if "*" in pattern:
            # å¤„ç†åˆ†ç‰‡æ–‡ä»¶
            import glob
            files = glob.glob(os.path.join(model_path, pattern))
            pytorch_files.extend(files)
        else:
            file_path = os.path.join(model_path, pattern)
            if os.path.exists(file_path):
                pytorch_files.append(file_path)
    
    if not pytorch_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æƒé‡æ–‡ä»¶")
        return
    
    print(f"æ‰¾åˆ° {len(pytorch_files)} ä¸ªæƒé‡æ–‡ä»¶:")
    for f in pytorch_files:
        print(f"  - {os.path.basename(f)}")
    
    # 3. åŠ è½½å’Œåˆå¹¶æ‰€æœ‰æƒé‡
    print("\n--- æ­¥éª¤ 3: åŠ è½½æƒé‡æ–‡ä»¶ ---")
    all_tensors = {}
    
    for weight_file in pytorch_files:
        print(f"åŠ è½½: {os.path.basename(weight_file)}")
        
        try:
            if weight_file.endswith('.safetensors'):
                # å¦‚æœå·²ç»æ˜¯ safetensors æ ¼å¼
                tensors = load_file(weight_file)
            else:
                # åŠ è½½ PyTorch æ ¼å¼ï¼Œä½¿ç”¨å®‰å…¨æ¨¡å¼
                try:
                    # å°è¯•ä½¿ç”¨ weights_only=True (éœ€è¦ PyTorch 2.6+)
                    tensors = torch.load(weight_file, map_location='cpu', weights_only=True)
                except TypeError:
                    # å¦‚æœä¸æ”¯æŒ weights_only å‚æ•°ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼ˆæœ‰å®‰å…¨é£é™©ï¼‰
                    print("âš ï¸  è­¦å‘Š: ä½¿ç”¨ä¼ ç»ŸåŠ è½½æ–¹æ³•ï¼Œå­˜åœ¨å®‰å…¨é£é™©")
                    tensors = torch.load(weight_file, map_location='cpu')
            
            # åˆå¹¶å¼ é‡
            for key, tensor in tensors.items():
                if key in all_tensors:
                    print(f"âš ï¸  è­¦å‘Š: é‡å¤çš„é”® {key}ï¼Œå°†è¢«è¦†ç›–")
                all_tensors[key] = tensor
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {weight_file}: {e}")
            continue
    
    if not all_tensors:
        print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•å¼ é‡")
        return
    
    print(f"âœ“ æ€»å…±åŠ è½½äº† {len(all_tensors)} ä¸ªå¼ é‡")
    
    # 3.5. è§£å†³å…±äº«å¼ é‡é—®é¢˜
    print("\n--- æ­¥éª¤ 3.5: è§£å†³å…±äº«å¼ é‡é—®é¢˜ ---")
    all_tensors = resolve_shared_tensors(all_tensors)
    
    # 4. è®¡ç®—æ€»å¤§å°å¹¶å†³å®šæ˜¯å¦åˆ†ç‰‡
    print("\n--- æ­¥éª¤ 4: è®¡ç®—æ¨¡å‹å¤§å° ---")
    total_size = 0
    for tensor in all_tensors.values():
        total_size += tensor.numel() * tensor.element_size()
    
    total_size_gb = total_size / (1024**3)
    print(f"æ¨¡å‹æ€»å¤§å°: {total_size_gb:.2f} GB")
    
    # è§£ææœ€å¤§åˆ†ç‰‡å¤§å°
    max_size_bytes = parse_size(max_shard_size)
    
    if total_size <= max_size_bytes:
        # å•æ–‡ä»¶ä¿å­˜
        print("ğŸ“¦ ä¿å­˜ä¸ºå•ä¸ª safetensors æ–‡ä»¶")
        output_file = os.path.join(output_path, "model.safetensors")
        save_file(all_tensors, output_file)
        print(f"âœ“ ä¿å­˜å®Œæˆ: model.safetensors")
        
        # åˆ›å»ºç´¢å¼•æ–‡ä»¶
        create_single_file_index(output_path, total_size)
        
    else:
        # åˆ†ç‰‡ä¿å­˜
        print(f"ğŸ“¦ æ¨¡å‹è¿‡å¤§ï¼Œå°†åˆ†ç‰‡ä¿å­˜ (æ¯ç‰‡æœ€å¤§ {max_shard_size})")
        shard_files = save_sharded_safetensors(all_tensors, output_path, max_size_bytes)
        
        # åˆ›å»ºåˆ†ç‰‡ç´¢å¼•æ–‡ä»¶
        create_sharded_index(output_path, shard_files, all_tensors)
    
    print(f"\nğŸ‰ è½¬æ¢å®Œæˆ! è¾“å‡ºç›®å½•: {output_path}")
    print("ç°åœ¨å¯ä»¥å®‰å…¨åœ°ä½¿ç”¨ safetensors æ ¼å¼åŠ è½½æ¨¡å‹äº†ã€‚")


def parse_size(size_str: str) -> int:
    """è§£æå¤§å°å­—ç¬¦ä¸²ï¼Œå¦‚ '5GB', '1TB' ç­‰"""
    size_str = size_str.upper().strip()
    
    if size_str.endswith('GB'):
        return int(float(size_str[:-2]) * 1024**3)
    elif size_str.endswith('MB'):
        return int(float(size_str[:-2]) * 1024**2)
    elif size_str.endswith('TB'):
        return int(float(size_str[:-2]) * 1024**4)
    else:
        return int(size_str)


def save_sharded_safetensors(tensors: Dict[str, torch.Tensor], output_path: str, max_size: int) -> list:
    """å°†å¼ é‡åˆ†ç‰‡ä¿å­˜ä¸ºå¤šä¸ª safetensors æ–‡ä»¶"""
    shard_files = []
    current_shard = {}
    current_size = 0
    shard_index = 1
    
    for name, tensor in tensors.items():
        tensor_size = tensor.numel() * tensor.element_size()
        
        # å¦‚æœå½“å‰åˆ†ç‰‡åŠ ä¸Šè¿™ä¸ªå¼ é‡ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰åˆ†ç‰‡
        if current_size + tensor_size > max_size and current_shard:
            shard_filename = f"model-{shard_index:05d}-of-{len(tensors):05d}.safetensors"
            shard_path = os.path.join(output_path, shard_filename)
            
            save_file(current_shard, shard_path)
            shard_files.append(shard_filename)
            print(f"âœ“ ä¿å­˜åˆ†ç‰‡: {shard_filename} ({current_size / 1024**2:.1f} MB)")
            
            current_shard = {}
            current_size = 0
            shard_index += 1
        
        current_shard[name] = tensor
        current_size += tensor_size
    
    # ä¿å­˜æœ€åä¸€ä¸ªåˆ†ç‰‡
    if current_shard:
        shard_filename = f"model-{shard_index:05d}-of-{len(tensors):05d}.safetensors"
        shard_path = os.path.join(output_path, shard_filename)
        
        save_file(current_shard, shard_path)
        shard_files.append(shard_filename)
        print(f"âœ“ ä¿å­˜åˆ†ç‰‡: {shard_filename} ({current_size / 1024**2:.1f} MB)")
    
    return shard_files


def create_single_file_index(output_path: str, total_size: int) -> None:
    """ä¸ºå•æ–‡ä»¶æ¨¡å‹åˆ›å»ºç´¢å¼•"""
    index = {
        "metadata": {"total_size": total_size},
        "weight_map": {"model.safetensors": "model.safetensors"}
    }
    
    index_path = os.path.join(output_path, "model.safetensors.index.json")
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, indent=2, ensure_ascii=False)


def create_sharded_index(output_path: str, shard_files: list, tensors: Dict[str, torch.Tensor]) -> None:
    """ä¸ºåˆ†ç‰‡æ¨¡å‹åˆ›å»ºç´¢å¼•æ–‡ä»¶"""
    weight_map = {}
    total_size = 0
    
    # é‡æ–°è®¡ç®—æ¯ä¸ªå¼ é‡åœ¨å“ªä¸ªåˆ†ç‰‡ä¸­
    current_shard_idx = 0
    current_size = 0
    max_size = parse_size("5GB")  # é»˜è®¤åˆ†ç‰‡å¤§å°
    
    for name, tensor in tensors.items():
        tensor_size = tensor.numel() * tensor.element_size()
        total_size += tensor_size
        
        if current_size + tensor_size > max_size and current_shard_idx < len(shard_files) - 1:
            current_shard_idx += 1
            current_size = 0
        
        weight_map[name] = shard_files[current_shard_idx]
        current_size += tensor_size
    
    index = {
        "metadata": {"total_size": total_size},
        "weight_map": weight_map
    }
    
    index_path = os.path.join(output_path, "model.safetensors.index.json")
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, indent=2, ensure_ascii=False)


def main():
    parser = argparse.ArgumentParser(description="å°† PyTorch æ¨¡å‹è½¬æ¢ä¸º safetensors æ ¼å¼")
    parser.add_argument("--input_path", type=str, default="/mnt/hdfs/selection/tt_stage2_model/general",
                       help="è¾“å…¥æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output_path", type=str, default="/mnt/hdfs/selection/tt_stage2_model/general_safe",
                       help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--trust_remote_code", default=True,
                       help="æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç ")
    parser.add_argument("--max_shard_size", type=str, default="5GB",
                       help="æœ€å¤§åˆ†ç‰‡å¤§å° (ä¾‹å¦‚: 5GB, 1TB)")
    parser.add_argument("--force", default=True,
                       help="å¼ºåˆ¶è¦†ç›–è¾“å‡ºç›®å½•")
    parser.add_argument("--use_model_api", action="store_true", default=True,
                       help="ä½¿ç”¨ save_model API (æ¨èï¼Œè‡ªåŠ¨å¤„ç†å…±äº«å¼ é‡)")
    parser.add_argument("--manual_mode", action="store_true",
                       help="å¼ºåˆ¶ä½¿ç”¨æ‰‹åŠ¨å¤„ç†æ¨¡å¼")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„ (å…è®¸ Hugging Face æ¨¡å‹åç§°)
    if not os.path.exists(args.input_path) and "/" not in args.input_path:
        # å¯èƒ½æ˜¯ Hugging Face æ¨¡å‹åç§°ï¼Œå°è¯•éªŒè¯
        try:
            from transformers import AutoConfig
            AutoConfig.from_pretrained(args.input_path, trust_remote_code=args.trust_remote_code)
            print(f"âœ“ æ£€æµ‹åˆ° Hugging Face æ¨¡å‹: {args.input_path}")
        except Exception as e:
            print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨ä¸”ä¸æ˜¯æœ‰æ•ˆçš„ Hugging Face æ¨¡å‹: {args.input_path}")
            print(f"é”™è¯¯: {e}")
            sys.exit(1)
    elif not os.path.exists(args.input_path):
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input_path}")
        sys.exit(1)
    
    # æ£€æŸ¥è¾“å‡ºè·¯å¾„
    if os.path.exists(args.output_path) and not args.force:
        print(f"âŒ è¾“å‡ºè·¯å¾„å·²å­˜åœ¨: {args.output_path}")
        print("ä½¿ç”¨ --force å‚æ•°å¼ºåˆ¶è¦†ç›–")
        sys.exit(1)
    
    try:
        # å¦‚æœæŒ‡å®šäº†æ‰‹åŠ¨æ¨¡å¼ï¼Œåˆ™ä¸ä½¿ç”¨ model API
        use_model_api = not args.manual_mode
        
        convert_pytorch_to_safetensors(
            model_path=args.input_path,
            output_path=args.output_path,
            trust_remote_code=args.trust_remote_code,
            max_shard_size=args.max_shard_size,
            use_model_api=use_model_api
        )
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()