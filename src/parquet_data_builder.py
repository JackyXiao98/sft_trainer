#!/usr/bin/env python3
"""
Parquetæ•°æ®é›†æ„å»ºå™¨
ç”¨äºå¤„ç†read_dataset.pyä¸­å®šä¹‰çš„13ä¸ªparquetæ•°æ®é›†ï¼Œç”Ÿæˆ53ä¸ªè®­ç»ƒé›†ï¼ˆ1 + 13*4ï¼‰
"""

import os
import yaml
import random
import pandas as pd
import pyarrow.parquet as pq
from typing import List, Dict, Tuple
from datasets import load_dataset, Dataset, concatenate_datasets
from transformers import AutoTokenizer
from rich.console import Console
from rich.progress import Progress, TaskID

from data_builder import DataBuilder

console = Console()

class ParquetDataBuilder(DataBuilder):
    """å¤„ç†parquetæ ¼å¼æ•°æ®é›†çš„DataBuilderå­ç±»"""
    
    def __init__(self, config_path: str = "configs/training_config.yaml"):
        super().__init__(config_path)
        
        # parquetæ•°æ®é›†é…ç½®
        self.base_dir = '/mnt/hdfs/selection/from_jiaxiang_wu/general_n_safety_datasets'
        self.datasets_to_load = {
            "é€šç”¨ä»»åŠ¡, non-thinkingæ¨¡å¼è®­ç»ƒæ•°æ® (5 domains)": [
                'tulu3_qwen3_2507_no_think_coding',
                'tulu3_qwen3_2507_no_think_instruction',
                'tulu3_qwen3_2507_no_think_knowledge',
                'tulu3_qwen3_2507_no_think_math',
                'tulu3_qwen3_2507_no_think_multilingual'
            ],
            "é€šç”¨ä»»åŠ¡, thinkingæ¨¡å¼è®­ç»ƒæ•°æ® (4 domains)": [
                'open_r1_qwen3_2507_0804_think_coding_8k',
                'open_r1_qwen3_2507_0804_think_math_8k',
                'tulu3_qwen3_2507_0805_think_knowledge_8k',
                'tulu3_qwen3_2507_0805_think_multilingual_8k'
            ],
            "å®‰å…¨å¯¹é½ä»»åŠ¡, ä¸­è‹±æ–‡ä¸ºä¸»çš„non-thinking & thinkingæ¨¡å¼è®­ç»ƒæ•°æ® (4 domains)": [
                'safety_cn_bias',
                'safety_tier1',
                'safety_tier2',
                'safety_tier3'
            ]
        }
        
        # è·å–æ‰€æœ‰æ•°æ®é›†åç§°åˆ—è¡¨
        self.all_dataset_names = []
        for category, datasets in self.datasets_to_load.items():
            self.all_dataset_names.extend(datasets)
        
        console.print(f"[green]å‘ç° {len(self.all_dataset_names)} ä¸ªparquetæ•°æ®é›†[/green]")
        console.print(f"å°†ç”Ÿæˆ {1 + len(self.all_dataset_names) * 4} ä¸ªè®­ç»ƒé›†")
    
    def load_parquet_safely(self, file_path: str) -> Tuple[Dataset, str]:
        """å®‰å…¨åŠ è½½parquetæ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ–¹æ³•"""
        console.print(f"  ğŸ”„ æ­£åœ¨åŠ è½½: {os.path.basename(file_path)}")
        
        # æ–¹æ³•1: ä½¿ç”¨datasetsåº“
        try:
            dataset = load_dataset('parquet', data_files=file_path)
            return dataset, "datasets"
        except Exception as e:
            console.print(f"  âš ï¸  datasetsæ–¹æ³•å¤±è´¥: {e}")
        
        # æ–¹æ³•2: ä½¿ç”¨pandas
        try:
            df = pd.read_parquet(file_path)
            dataset = Dataset.from_pandas(df)
            return {"train": dataset}, "pandas"
        except Exception as e:
            console.print(f"  âš ï¸  pandasæ–¹æ³•å¤±è´¥: {e}")
        
        # æ–¹æ³•3: ä½¿ç”¨pyarrow
        try:
            table = pq.read_table(file_path)
            df = table.to_pandas()
            dataset = Dataset.from_pandas(df)
            return {"train": dataset}, "pyarrow"
        except Exception as e:
            console.print(f"  âš ï¸  pyarrowæ–¹æ³•å¤±è´¥: {e}")
        
        return None, "failed"
    
    def load_source_datasets(self) -> Dict[str, Dataset]:
        """åŠ è½½parquetæºæ•°æ®é›†"""
        console.print("[blue]æ­£åœ¨åŠ è½½parquetæ•°æ®é›†...[/blue]")
        
        datasets = {}
        
        for category, file_list in self.datasets_to_load.items():
            console.print(f"\n[cyan]ç±»åˆ«: {category}[/cyan]")
            
            for file_name in file_list:
                file_path = os.path.join(self.base_dir, f'{file_name}.parquet')
                
                if not os.path.exists(file_path):
                    console.print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    continue
                
                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
                console.print(f"  ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
                
                dataset_result, method = self.load_parquet_safely(file_path)
                
                if dataset_result is None:
                    console.print(f"  âŒ æ— æ³•åŠ è½½æ•°æ®é›†: {file_name}")
                    continue
                
                console.print(f"  âœ… æˆåŠŸåŠ è½½ (æ–¹æ³•: {method})")
                
                # è·å–è®­ç»ƒæ•°æ®é›†
                train_dataset = dataset_result['train']
                console.print(f"  ğŸ“Š æ ·æœ¬æ•°é‡: {len(train_dataset):,}")
                
                # å­˜å‚¨æ•°æ®é›†
                datasets[file_name] = train_dataset
        
        console.print(f"\n[green]æˆåŠŸåŠ è½½ {len(datasets)} ä¸ªæ•°æ®é›†[/green]")
        return datasets
    
    def create_base_datasets(self, source_datasets: Dict[str, Dataset]) -> Dict[str, Dataset]:
        """åˆ›å»ºåŸºç¡€æ•°æ®é›†ï¼ˆæ¯ä¸ªæºæ•°æ®é›†ä¸€ä¸ªï¼‰"""
        console.print("[blue]åˆ›å»ºåŸºç¡€æ•°æ®é›†...[/blue]")
        
        base_datasets = {}
        
        console.print(f"[blue]å¼€å§‹å¤„ç† {len(source_datasets)} ä¸ªæ•°æ®é›†...[/blue]")
        
        for i, (dataset_name, dataset) in enumerate(source_datasets.items(), 1):
            console.print(f"[cyan]({i}/{len(source_datasets)}) å¤„ç†æ•°æ®é›†: {dataset_name}[/cyan]")
            
            # æŒ‰tokené™åˆ¶é‡‡æ ·ï¼ˆå†…éƒ¨ä¼šæ˜¾ç¤ºè¿›åº¦ï¼‰
            sampled_dataset = self.sample_by_token_count(
                dataset, self.train_token_limit, dataset_name
            )
            
            base_datasets[dataset_name] = sampled_dataset
            console.print(f"[green]âœ“ å®Œæˆæ•°æ®é›† {dataset_name} çš„å¤„ç†[/green]")
        
        console.print(f"[blue]æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼[/blue]")
        return base_datasets
    
    def create_validation_datasets(self, source_datasets: Dict[str, Dataset]) -> Dict[str, Dataset]:
        """åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆä»13ä¸ªæ•°æ®é›†ä¸­é‡‡æ ·ï¼‰"""
        console.print("[blue]åˆ›å»ºéªŒè¯æ•°æ®é›†...[/blue]")
        
        validation_datasets = {}
        
        console.print(f"[blue]å¼€å§‹ä¸º {len(source_datasets)} ä¸ªæ•°æ®é›†åˆ›å»ºéªŒè¯é›†...[/blue]")
        
        for i, (dataset_name, dataset) in enumerate(source_datasets.items(), 1):
            console.print(f"[cyan]({i}/{len(source_datasets)}) ä¸º {dataset_name} åˆ›å»ºéªŒè¯é›†[/cyan]")
            
            # æŒ‰tokené™åˆ¶é‡‡æ ·éªŒè¯é›†ï¼ˆå†…éƒ¨ä¼šæ˜¾ç¤ºè¿›åº¦ï¼‰
            val_dataset = self.sample_by_token_count(
                dataset, self.val_token_limit, f"{dataset_name}_val"
            )
            
            validation_datasets[f"{dataset_name}_val"] = val_dataset
            console.print(f"[green]âœ“ å®ŒæˆéªŒè¯é›† {dataset_name}_val çš„åˆ›å»º[/green]")
        
        console.print(f"[blue]æ‰€æœ‰éªŒè¯é›†åˆ›å»ºå®Œæˆï¼[/blue]")
        return validation_datasets
    
    def create_training_variants(self, base_datasets: Dict[str, Dataset]) -> Dict[str, Dataset]:
        """åˆ›å»ºè®­ç»ƒå˜ä½“ï¼š1ä¸ªå…¨é‡ + 13*4ä¸ªå˜ä½“æ‰°åŠ¨"""
        console.print("[blue]åˆ›å»ºè®­ç»ƒå˜ä½“...[/blue]")
        
        training_datasets = {}
        dataset_names = list(base_datasets.keys())
        
        # 1. åˆ›å»ºå…¨é‡æ•°æ®é›†ï¼ˆæ‰€æœ‰13ä¸ªæ•°æ®é›†çš„ç»„åˆï¼‰
        console.print("åˆ›å»ºå…¨é‡è®­ç»ƒæ•°æ®é›†...")
        all_datasets = list(base_datasets.values())
        if all_datasets:
            full_dataset = concatenate_datasets(all_datasets)
            training_datasets["full_dataset"] = full_dataset
            console.print(f"[green]å…¨é‡æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(full_dataset):,} æ ·æœ¬[/green]")
        
        # 2. ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»º4ç§å˜ä½“æ‰°åŠ¨ï¼ˆ1/3, 1/2, 2x, 3xï¼‰+ æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        console.print(f"\nå¼€å§‹ä¸º {len(dataset_names)} ä¸ªæ•°æ®é›†åˆ›å»ºå˜ä½“æ‰°åŠ¨...")
        
        for i, target_dataset_name in enumerate(dataset_names, 1):
            console.print(f"\n[cyan]({i}/{len(dataset_names)}) ä¸º {target_dataset_name} åˆ›å»º4ç§å˜ä½“æ‰°åŠ¨[/cyan]")
            
            target_dataset = base_datasets[target_dataset_name]
            
            # è·å–æ‰€æœ‰å…¶ä»–æ•°æ®é›†
            other_datasets = [base_datasets[name] for name in dataset_names if name != target_dataset_name]
            
            # åˆå¹¶æ‰€æœ‰å…¶ä»–æ•°æ®é›†
            if other_datasets:
                other_combined = concatenate_datasets(other_datasets)
            else:
                # å¦‚æœæ²¡æœ‰å…¶ä»–æ•°æ®é›†ï¼Œä½¿ç”¨ç©ºæ•°æ®é›†
                other_combined = Dataset.from_dict({})
            
            # åˆ›å»º4ç§å˜ä½“æ‰°åŠ¨
            variants = self._create_dataset_variants(target_dataset, other_combined, target_dataset_name)
            training_datasets.update(variants)
        
        console.print(f"\n[green]åˆ›å»ºäº† {len(training_datasets)} ä¸ªè®­ç»ƒæ•°æ®é›†[/green]")
        return training_datasets
    
    def _create_dataset_variants(self, target_dataset: Dataset, other_combined: Dataset, dataset_name: str) -> Dict[str, Dataset]:
        """ä¸ºç‰¹å®šæ•°æ®é›†åˆ›å»º4ä¸ªå˜ä½“æ‰°åŠ¨"""
        variants = {}
        
        # 1/3 å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„1/3 + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        if len(target_dataset) >= 3:
            subset_1_3 = target_dataset.select(random.sample(range(len(target_dataset)), len(target_dataset) // 3))
            if len(other_combined) > 0:
                variants[f'{dataset_name}_1_3'] = concatenate_datasets([subset_1_3, other_combined])
            else:
                variants[f'{dataset_name}_1_3'] = subset_1_3
            console.print(f"  [green]{dataset_name}_1_3 åˆ›å»ºå®Œæˆ: {len(variants[f'{dataset_name}_1_3']):,} æ ·æœ¬[/green]")
        
        # 1/2 å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„1/2 + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        if len(target_dataset) >= 2:
            subset_1_2 = target_dataset.select(random.sample(range(len(target_dataset)), len(target_dataset) // 2))
            if len(other_combined) > 0:
                variants[f'{dataset_name}_1_2'] = concatenate_datasets([subset_1_2, other_combined])
            else:
                variants[f'{dataset_name}_1_2'] = subset_1_2
            console.print(f"  [green]{dataset_name}_1_2 åˆ›å»ºå®Œæˆ: {len(variants[f'{dataset_name}_1_2']):,} æ ·æœ¬[/green]")
        
        # 2x å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„2å€ + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        double_dataset = concatenate_datasets([target_dataset, target_dataset])
        if len(other_combined) > 0:
            variants[f'{dataset_name}_2x'] = concatenate_datasets([double_dataset, other_combined])
        else:
            variants[f'{dataset_name}_2x'] = double_dataset
        console.print(f"  [green]{dataset_name}_2x åˆ›å»ºå®Œæˆ: {len(variants[f'{dataset_name}_2x']):,} æ ·æœ¬[/green]")
        
        # 3x å˜ä½“ï¼šç›®æ ‡æ•°æ®é›†çš„3å€ + æ‰€æœ‰å…¶ä»–æ•°æ®é›†
        triple_dataset = concatenate_datasets([target_dataset, target_dataset, target_dataset])
        if len(other_combined) > 0:
            variants[f'{dataset_name}_3x'] = concatenate_datasets([triple_dataset, other_combined])
        else:
            variants[f'{dataset_name}_3x'] = triple_dataset
        console.print(f"  [green]{dataset_name}_3x åˆ›å»ºå®Œæˆ: {len(variants[f'{dataset_name}_3x']):,} æ ·æœ¬[/green]")
        
        return variants
    
    def build_all_datasets(self):
        """æ„å»ºæ‰€æœ‰æ•°æ®é›†"""
        console.print("[bold green]å¼€å§‹æ„å»ºparquetæ•°æ®é›†...[/bold green]")
        
        # 1. åŠ è½½æºæ•°æ®é›†
        source_datasets = self.load_source_datasets()
        if not source_datasets:
            console.print("[red]æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æºæ•°æ®é›†ï¼Œé€€å‡º[/red]")
            return
        
        # 2. åˆ›å»ºåŸºç¡€æ•°æ®é›†
        base_datasets = self.create_base_datasets(source_datasets)
        
        # 3. åˆ›å»ºéªŒè¯æ•°æ®é›†
        validation_datasets = self.create_validation_datasets(source_datasets)
        
        # 4. åˆ›å»ºè®­ç»ƒå˜ä½“
        training_datasets = self.create_training_variants(base_datasets)
        
        # 5. ä¿å­˜æ•°æ®é›†
        self.save_datasets(training_datasets, validation_datasets)
        
        console.print("[bold green]parquetæ•°æ®é›†æ„å»ºå®Œæˆï¼[/bold green]")
        console.print(f"è®­ç»ƒæ•°æ®é›†: {len(training_datasets)} ä¸ª")
        console.print(f"éªŒè¯æ•°æ®é›†: {len(validation_datasets)} ä¸ª")


def main():
    """ä¸»å‡½æ•°"""
    console.print("[bold blue]Parquetæ•°æ®é›†æ„å»ºå™¨[/bold blue]")
    
    builder = ParquetDataBuilder()
    builder.build_all_datasets()


if __name__ == "__main__":
    main()